%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% I, the copyright holder of this work, release this work into the
%% public domain. This applies worldwide. In some countries this may
%% not be legally possible; if so: I grant anyone the right to use
%% this work for any purpose, without any conditions, unless such
%% conditions are required by law.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[
  color, %% This option enables colorful typesetting. Replace with
         %% `monochrome`, if you are going to print the thesis on
         %% a monochromatic printer.
  table, %% Causes the coloring of tables. Replace with `notable`
         %% to restore plain tables.
  lof,   %% Prints the List of Figures. Replace with `nolof` to
         %% hide the List of Figures.
  lot,   %% Prints the List of Tables. Replace with `nolot` to
         %% hide the List of Tables.
  %% More options are listed in the class documentation at
  %% <http://mirrors.ctan.org/macros/latex/contrib/fithesis/fithesis/guide/mu/fi.pdf>.
]{fithesis3}
%% The following section sets up the locales used in the thesis.
\usepackage[resetfonts]{cmap} %% We need to load the T2A font encoding
\usepackage[T1,T2A]{fontenc}  %% to use the Cyrillic fonts with Russian texts.

\usepackage[
  main=english, %% By using `czech` or `slovak` as the main locale
                %% instead of `english`, you can typeset the thesis
                %% in either Czech or Slovak, respectively.
  german, russian, czech, slovak %% The additional keys allow
]{babel}        %% foreign texts to be typeset as follows:
%%
%%   \begin{otherlanguage}{german}  ... \end{otherlanguage}
%%   \begin{otherlanguage}{russian} ... \end{otherlanguage}
%%   \begin{otherlanguage}{czech}   ... \end{otherlanguage}
%%   \begin{otherlanguage}{slovak}  ... \end{otherlanguage}
%%
%% For non-Latin scripts, it may be necessary to load additional
%% fonts:
\usepackage{paratype}
\def\textrussian#1{{\usefont{T2A}{PTSerif-TLF}{m}{rm}#1}}
%%
%% The following section sets up the metadata of the thesis.
\thesissetup{
    university    = mu,
    faculty       = fi,
    type          = bc,
    author        = Samuel Petrovic,
    gender        = m,
    advisor       = Adam Rambousek,
    title         = {The effect of aging on filesystems performance},
    TeXtitle      = {The effect of aging on filesystems performance},
    keywords      = {filesystem, xfs, IO operation, aging, fragmentation ...},
    TeXkeywords   = {filesystem, xfs, IO operation, aging, fragmentation ...},
}
\thesislong{abstract}{
    This is the abstract of my thesis, which can

    span multiple paragraphs.
}
\thesislong{thanks}{
    This is the acknowledgement for my thesis, which can

    span multiple paragraphs.
}
%% The following section sets up the bibliography.
\usepackage{csquotes}
\usepackage[              %% When typesetting the bibliography, the
  backend=biber,          %% `numeric` style will be used for the
  style=numeric,          %% entries and the `numeric-comp` style
  citestyle=numeric-comp, %% for the references to the entries. The
  sorting=none,           %% entries will be sorted in cite order.
  sortlocale=auto         %% For more unformation about the available
]{biblatex}               %% `style`s and `citestyles`, see:
%% <http://mirrors.ctan.org/macros/latex/contrib/biblatex/doc/biblatex.pdf>.
\addbibresource{example.bib} %% The bibliograpic database within
                          %% the file `example.bib` will be used.
\usepackage{makeidx}      %% The `makeidx` package contains
\makeindex                %% helper commands for index typesetting.
%% These additional packages are used within the document:
\usepackage{paralist}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{url}
\usepackage{menukeys}
\begin{document}
\chapter{Introduction}
This thesis will debate the effect of aging and fragmentation on filesystem performance, specifically ext4 and xfs.

Performance of Red Hat supported filesystems is closely watched by our team, but the effect of aging is not included in the testing matrix and is quite lightly understood in general.

Therefore, this thesis will try to shed some light as what the effect could possibly be as well as to design somewhat conceptual way of exploring this field further.

Possible other chapters to explain terms: journaling filesystem, allocation groups, B+ trees

\chapter{Filesystems and used tools}
\section{Filesystems}
File system is a set of tools, methods, logic and structure to control how to store and retreive data on a storage, e.g. a device. It is sometimes called 'bookkeeper' of operational system.
As analogy to paper-based systems, basic user-accesed units are called files, which could be clustered into directories.

The system stores files either continuously or scattered across filesystem in form of extents. The basic accesed data unit is called a block, which size can be set to various sizes. Block can be either free or used, meaning there is some data stored or not. Information about how many blocks does a file occupy, as well as other information like date of creation, date of last access or access permissions is called metadata, e.g. data about stored data.

Depending on filesystem, these metadata are stored in various ways (different tables or arrays), but on modern file systems, there are objects called inodes (i stands for index). Each file a filesystem manages is associated with an inode, every inode has its number in an inode table.

*este nieco o bitovych mapach volneho miesta.

In this thesis, targeted filesystems will be UNIX XFS and EXT4, which are main Red Hat supported filesystems. These file systems belong to Journaling filesystems group.

Journaling file system keeps a structure called journal, which is a buffer of changes not yet commited to the file system. After system failure, these planned changes can be easily read from the journal, thus making the file system easily fully operational, and in correct(consistent) state again.

*IO scheduler
\section{XFS}
XFS is a 64-bit journaling file system created by Silicon Graphics, Inc(SGI) in 1993. It is known for great performance in execution of paralel I/O operations, because of its architecture based on allocation groups.

Allocation groups are euqally sized linear regions within file system. Each allocation group manages its own inodes and free space, therefore increasing parallelism.
Architecture of this design enables for significant scalability of bandwidth, threads, and size of filesystem, as well as its files, simply because multiple processes and threads can access the file system simultaneously.

XFS allocates space as extents stored in pairs of B+ trees, each pair for each allocation group (imrpoving performance especially when handling large files). One of the B+ trees is indexed by the length of the free extents, while the other is indexed by the starting block of the free extents. This dual indexing scheme allows for the highly efficient location of free extents for file system operations.

Extent describes one or more contiguous blocks, which can considerably shorten list of blocks.
 
Metadata journaling ensures consistency of data in case of emergency situations as f.e. system crash.

Prevention of file system fragmentation consist mainly of \textit{delayed allocation} feature as well as online defragmentation(\textit{xfs\_fsr}), that can turururu

Delayed allocation, also called \textit{allocate-on-flush} is a feature that, when a file is written to the buffer cache, substracts space from the free-space counter, but won't allocate the free-space bitmap. The data is held in memory, instead, until it must be flushed (to storage) because of memory pressure when calling Unix \textit{sync}, or when flushing dirty buffers. This approach improves the chance, that the file will be written in a contiguous group of blocks, avoiding fragmentation and reducing CPU usage as well.

\section{EXT4}
Ext4, also called \textit{fourth extended filesystem} is a 48-bit journaling file system, developed as successor of ext3 for linux kernel, improving reliability and performance features.

Traditionally, ext* systems use an indirect block mapping sheme. Such an approach is generaly inefficient for large files, on operations like deleting or truncating. Ext4, as well as XFS use approach of \textit{extents}, which positively affect performance and encourage continuous layouts.

When allocating, ext4 use multiblock allocation, which is more efficient way than one block allocation at time, which is present in earlier ext* file systems. Multiblock allocation has far better performance, particulary when in use with delayed allocation and extents.

Similary as xfs, ext4 use delayed allocation design, to increase performance, especially when in use with multiblock allocation and extent-based approach, also reducing fragmentation on the device. For cases of fragmentation that still occur, ext4 provide support for online defragmentation and \textit{e4defrag} tool to defragment either single file, or whole filesystem.

\section{FIO}
Flexible Input/Output tool is a IO workload generator written by Jens Axboe. It is a tool well known for it's flexibility as well as large group of contributors and users.


\section{Fs-drift}
fs-drift is a workload aging test written by Ben England. It relies on randomly mixed requests generated according to options. These requests can be writes, reads, creates, appends or deletes.

At the beginning of run time, the top directory is empty, and therefore \textit{create} requests success the most, other requests, such as \textit{read} or \textit{delete}, will fail because of lack of files and small probability of randomly choosing existing one. 

Over time, as the filesystem grows, \textit{create} requests began to fail and other requests succede more. Finally, filesystem will reach a state of equilibrium, when requests are equaly likely to execute. From this point, the filesystem will not grow anymore, and the test will run until one of the \textit{STOP} conditions are met (specified with parameters).

\section{Snapshots}
From time reasons, but to ensure stability of results as well, I needed a solution, how to test an aged filesystem with possibility to recover to the pre-test state. For this matter, I made use of kernel native tools for inspecting filesystem in case of corruption or other forensic reasons.

Unfortunately, I had to use different tools for either filesystem. For EXT4, the tool is e2image[] and for XFS xfs-metadump[].

Both of these tools have possibility to store only metadata of files, not the actual data, which is a great option for this thesis, as it does not need the randomly generated data inside of files. Moreso it reduces the size of a snapshot to a mere fraction of the acutal volume, and the process of storing the snapshot si very quick.


\chapter{Implementation}
\section{filesystem\_ager}
This aging tool is a simple approach to write and remove many files of random size.

The tool consist of three scripts and one common library called \textit{functions}. The scripts are named \textit{filesystem\_ager.py}, \textit{fio\_config\_generator.py} and \textit{random\_deletor.py}.

The workflow consist of calling filesystem\_ager, with desired parameters. Script manages triggering fio\_config\_generator, calling fio tool on generated config and triggering random deletor. These three actions are repeated given number of times.
Parameters of filesystem\_ager are: 
\begin{compactenum}
  \item Total desired size do be written in one cycle
  \item Denominator of total desired size (Total desired size will be divided by this number)
  \item Range of size of written files
  \item Number of cycles
\end{compactenum}

Although FIO tool has some parameters to randomize the size of files which are written, the management of file sizes and randomisation, as well as naming of files is handled by fio\_config\_generator instead, to provide more control over those qualities.
Parameters of this script are:
\begin{compactenum}
  \item Total deisred size to be written
  \item Range of size of written files
\end{compactenum}

The script will generate global settings of a workload, then proceeds to generate jobs for every file that will be written. File size is always the name of that file, and these are gathered to a list, then list of generated files is returned and script ends. Including file size in its name, as well as indexation of files will help effectively search and delete files in the random deletion process, without need to search for files on the disk and examine them for size. Simplistic approach in fio config will hopefully result in compatibility and reliability in use with any fio version.

After config file is generated, filesystem\_ager will run fio tool on generated config and therefore, files are written on the device.

The removing of files is handled by random\_deletor script. Its parameters are:
\begin{compactenum}
  \item Total written size
  \item Denominator of total size
  \item Range of size of written files
  \item Number of existent files
\end{compactenum}

If denominator equals zero, random\_deletor wont remove any files and will return empty list. Otherwise, desired range of deletion is estimated. Random\_deletor then proceeds to remove files while desired volume is not deleted. Files are randomly selected through choosing random integer from zero to number of existent files. This step may seem inefficient, but with large amounts of generated files, the time to perform succesfull selection will not change dramatically. Selected file name is then parsed for size information, and if it fits into desired volume to remove, it is deleted, through subprocess command. Names of removed files are gathered in a list and returned.

Number of deleted files is substracted from number of existent files.
filesystem\_ager then sums up deleted volume, log it as well as other information and triggers the cycle again.

However, after few runs, I decided not to use this approach for actual aging, because the time needed to fill and appropriately age the filesystem simply took very long. Instead, I was looking for other, allready created tools I could use.


\section{recipe\_fio}
Measuring a performance is done by a tool I developed, recipe\_fio. Similar to filesytem\_ager, recipe\_fio use fio tool to handle desired IO operations, but instead of focusing on filling the filesystem, the script use measurement features of fio, which consist of performing IO operations and reporting results.

*talk about used recipe

For purposes of this thesis, I let recipe\_fio to report bandwidth and operations per second(IOPS).

The main script receives slightly enhanced fio configuration file, enriched of some non-fio parameters, which are used by test only. These parameters are:
\begin{compactenum}
  \item used filesystem
  \item number of test repetitions. For statistical stability, I decided to run the test several times under same conditions.
  \item specifying a snapshot to be tested
  \item flag which represents whether or not to rsync data on a result-storing server
  \end{compactenum}

After compiling, tool parses the parameters and gathers information about system, which consist of: version of kernel, time and date, hostname, RHEL compose, memory, kernel, mount, system info, system variables and fio version.

Then it proceeds to set environment for testing by:

\begin{compactenum}
  \item Installing fio tool
  \item Creating directory for results
  \item Loading given snapshot on a device
  \item Randomly removing volume from the device to make room for test
\end{compactenum}

Volume is randomly removed using random\_delete\_volume.py script. This scriptt globs all files in the filesystem, retrieves information about used volume as well as it's overall size. Then proceeds to randomly choosing files to delete and stops when desired volume is freed. The approach of recursively globbing all files may be inefficient, but this way, we can be sure, that volume is deleted from whole device evenly.

Python script run\_tests.py, manages to parse recipe parameter, resulting in creating one or several fio configuration files in the directory, further adding logging parameters to them. Then for every created file, directory on the desired medium is created and fio tool is triggered. If the script succesfully ends, file OK is created in the results directory.

When the measurement is over, bash script generate the name of results, which consist of:
\begin{compactenum}
  \item time
  \item date
  \item used snapshot
  \item version of kernel
  \item version of RHEL compose
\end{compactenum}

Then proceeds to compress results into tar archive with generated name, and according to -g flag will, or will not, rsync the result onto the data server.
\section{Inspecting filesystem}
For determining some overall idea about an extent to which is the filesystem aged or dirty, I wrote scripts that generate histograms representing fragmentation of used space as well as fragmentation of free space. Both scripts use common linux tools and pyplot to generate the graphics. Both scripts can display linear or logarhytmic Y scale.


Script extent\_distribution.py makes use of xfs\_io fiemap tool, which is a tool to display extent distribution of a given file and works correctly even for ext* filesystems.

The script will first recursively crawls the whole filesystem from given top folder and makes a list of all files. Fiemap is then run over every file separately. 

The only data, that are then parsed from the output, is how many non-contigous extents does the file have. These integers are aggregated to a single list, from which are then counted, and final histogram is made.

Script free\_space\_fragmentation.py use the tool e2freefrag, which runs over a device, and outputs the histogram of free space fragmentation in texutal form. Script will store this output and then easily parse the histogram and aggregate the data into a graphic form.

\section{Aging recipes}
To determine which fs-drift settings will be the most fitting for purposes of this thesis, I wrote a small python script fs-drift\_matrix.

It is capable of taking matrix of possible fs-drift paramteres from \textit{json} file and then run it on a device.

After every run, histograms-generating scripts are triggered to store histograms of \textit{free space} and \textit{used space} fragmentation. Also outputs of the \textit{fs-drift} script and \textit{df} command are logged.

As the creator states in README, to fill up a filesystem, maximum number of files and mean size of file should be defined such that the product is greater than the available space.

For the purpose of this thesis, desired usage is 60\%-100\% with enough fragmentation to consider the device aged.

\section{Data processing}
\chapter{Testing environment}
The aging process took place on an ibm3250 machine with following parameters:
\begin{compactenum}
  \item Intel(R) Xeon(R) CPU, X2460 (2.80Ghz, ??? Cache,8 cores)
  \item RAM 10GB ???Mhz DDR? ???
  \item 4x300GB SAS disks + 1x50GB Sas disk ???
\end{compactenum}

THIS IS AN EXAMPLE, HOW SHOULD SCECIFICS OF A MACHINE LOOK

2 x Intel Xeon E5-2620 (2.0GHz, 15MB Cache, 6-cores)
Intel C602 Chipset
Memory - 16GB (2 x 8GB) 1333Mhz DDR3 Registered RDIMMs
 CentOS 6.3 64-Bit
100GB Micron RealSSD P400e Boot SSD
LSI 9207-8i SAS/SATA 6.0Gb/s HBA (For benchmarking SSDs or HDDs)

The system installed on machine was ----- with kernel 3.10.0-229.el7.x86\_64 

I created a two volume groups, G1 and G2. Each group have a pair of 300GB disks with striping of 2. This setting allows to double the speed of IO operations.

On each volume group i created logical volume of 100GB.
\section{HDD and SSD}
HDD is a rotational disk, which requires specific approach from kernel, to ensure the lowest possible seek time. Seek time is a time for moving parts of the device to find next relevant block of data. This affect overall performance greatly, because with large fragmentation, seek time becomes quite high.

As for SSD, this type of device does not have any moving parts, which make perform really well. One of the problems, however, is limited lifecycle of memory cells. SSD manufacturers deal with this problem by adding controler with its own scheduler, which make sure, no parts of the device are used significantly more than other parts.

When aging the filesystems, I expect for those grown on HDD to perform significantly slower after aging process, and I expect SSD filesystems not to be affected at all, or maybe significantly less.

\chapter{Results}
The output of result generator is a htlm report summarising all information about system, links to raw data and charts of measured values.
*talk about highcharts and how do you represent data

\section{Comparing against fresh filesystems}
\section{Comparing performance of Ext4 and XFS}
\section{Comparing overall state of aged EXT4 and XFS}
\section{Comparing rotational and solid state disks}
\chapter{Conclusion}
Here I will admit, that these results were not really surprising and ABSOLUTELY no breakthrough, however, as noone really research this branch of QE, the results are definitely a step further in this field.
\end{document}
