%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% I, the copyright holder of this work, release this work into the
%% public domain. This applies worldwide. In some countries this may
%% not be legally possible; if so: I grant anyone the right to use
%% this work for any purpose, without any conditions, unless such
%% conditions are required by law.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[
  color, %% This option enables colorful typesetting. Replace with
         %% `monochrome`, if you are going to print the thesis on
         %% a monochromatic printer.
  table, %% Causes the coloring of tables. Replace with `notable`
         %% to restore plain tables.
  lof,   %% Prints the List of Figures. Replace with `nolof` to
         %% hide the List of Figures.
  lot,   %% Prints the List of Tables. Replace with `nolot` to
         %% hide the List of Tables.
  %% More options are listed in the class documentation at
  %% <http://mirrors.ctan.org/macros/latex/contrib/fithesis/fithesis/guide/mu/fi.pdf>.
]{fithesis3}
%% The following section sets up the locales used in the thesis.
\usepackage[resetfonts]{cmap} %% We need to load the T2A font encoding
\usepackage[T1,T2A]{fontenc}  %% to use the Cyrillic fonts with Russian texts.

\usepackage[
  main=english, %% By using `czech` or `slovak` as the main locale
                %% instead of `english`, you can typeset the thesis
                %% in either Czech or Slovak, respectively.
  german, russian, czech, slovak %% The additional keys allow
]{babel}        %% foreign texts to be typeset as follows:
%%
%%   \begin{otherlanguage}{german}  ... \end{otherlanguage}
%%   \begin{otherlanguage}{russian} ... \end{otherlanguage}
%%   \begin{otherlanguage}{czech}   ... \end{otherlanguage}
%%   \begin{otherlanguage}{slovak}  ... \end{otherlanguage}
%%
%% For non-Latin scripts, it may be necessary to load additional
%% fonts:
\usepackage{paratype}
\def\textrussian#1{{\usefont{T2A}{PTSerif-TLF}{m}{rm}#1}}
\usepackage{xcolor} 
\newcommand{\todo}[1]{\textcolor{red}{\textbf{#1}}}
\usepackage{listings}
%%
%% The following section sets up the metadata of the thesis.
\thesissetup{
    university    = mu,
    faculty       = fi,
    type          = bc,
    author        = Samuel Petrovic,
    gender        = m,
    advisor       = Adam Rambousek,
    title         = {The effects of age on file system performance},
    TeXtitle      = {The effects of age on file system performance},
    keywords      = {filesystem, xfs, IO operation, aging, fragmentation ...},
    TeXkeywords   = {filesystem, xfs, IO operation, aging, fragmentation ...},
}
\thesislong{abstract}{
    This is the abstract of my thesis, which can

    span multiple paragraphs.
}
\thesislong{thanks}{
    This is the acknowledgement for my thesis, which can

    span multiple paragraphs.
}
%% The following section sets up the bibliography.
\usepackage{csquotes}
\usepackage[              %% When typesetting the bibliography, the
  backend=biber,          %% `numeric` style will be used for the
  style=numeric,          %% entries and the `numeric-comp` style
  citestyle=numeric-comp, %% for the references to the entries. The
  sorting=none,           %% entries will be sorted in cite order.
  sortlocale=auto         %% For more unformation about the available
]{biblatex}               %% `style`s and `citestyles`, see:
%% <http://mirrors.ctan.org/macros/latex/contrib/biblatex/doc/biblatex.pdf>.
\addbibresource{example.bib} %% The bibliograpic database within
                          %% the file `example.bib` will be used.
\usepackage{makeidx}      %% The `makeidx` package contains
\makeindex                %% helper commands for index typesetting.
%% These additional packages are used within the document:
\usepackage{paralist}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{url}
\usepackage{menukeys}
\begin{document}
\chapter{Introduction}
We live in an age of information. Hardware and software technology improves every day. There is an increasing need to wrok with larger and larger databases, multimedia applications and to store great amount of data. This cause great pressure on performance of storing and retrieving information, e.g. input and output (I/O) operations.

The part of an operation system that handles communication with a physical device is called a file system. Because of increasing demands on performance  of I/O operations, there has been quite wide tehcnological progress in approach to performance issues.

In past years, great many tools and tests (e.g. benchmarks) were developed as means for users and researchers to explore behavior and to measure performance of file systems. New versions of file systems are released very often (almost weekly) and therefore there has been an pressure on quality engineers to evaluate new versions fast enough.

The standard work flow consist of running test on freshly installed operation system and newly created file system without running other applications, which might cause noise in measurements and eventually invalid results.

Although this work flow shows great results, it only gives us a general idea about how does file system perform at early stages of its life cycle. Results of these tests are therefore quite unrealistic and won't address the behavior of file system after long-term usage.

Prolonged usage of file system impel it to do more and more optimisations with impact on its perfmormance. The more files has been created, deleted or expanded, the free space of file system is progressingly more fragmented. Such fragmentation means, that blocks of data are scattered through physical device which results in degradation of performance.

The ideal approach for research pointed on effect on age of file system would be to put file system under stress conditions for a few years or months, gathering information in the process, however, such approach is apparently impractical or even impossible, because the demand has to be satisfied in matter of weeks to respond to new versions of file systems.

\chapter{State of art}
% We live in an age of information. Hardware and software technology improves every day. There is a need to work with large databases, multi-media applications and to store great amount of data on a memory device. This cause great pressure on performance of storing and retrieving information, viz. input and output(I/O) operations.

% Part of an operation system that handles communication with a physical device is called a file system. Because of such increasing requirement on performance of I/O operations, there has been quite wide technological progress in approach to performance issues. 

% In past years, great many tools and tests (e.g. benchmarks) were developed as means for users and researchers to explore behavior and measure performance of file systems. These usually consist of putting file system under some kind of stress, gathering different measurements in the process and computing overall summary after the test is done. Measured parameters are usually bandwidth, latency and IO operations per second.

% The benchmark should be highly configurable, so the researchers can simulate various workloads that would mimic real life scenarios. Stability and reproducibility of tests are also required, if the research is meant to be objective and to have some academic value about file system itself.

% The standard workflow is tu run benchmark on a clean instance of operation system without other applications running, to avoid noise, and on clean instance of researched file system.

% While this approach brings great results, with correct configuration of the benchmark, it only gives us a general idea about how does file system perform in early stage of it's usage. 

% However, there is a growing demand from users as well as from developers to explore behavior and performance of a file system, that has been running under some defined conditions for longer period of time, months or even years.

% Prolonged usage of file system impel it to do more and more optimisations. When there is a lot of free space, there is generally no need for complicated approach. But with more files being deleted, created and expanded, the free space begins to be more and more fragmented. This then leads for files to be spread non contiguously across the physical device.

% This is a problem mainly with regard to devices with moving parts, such as widely used hard disk drive. When working with IO operations on this type of device, the fragmentation cause larger seek times, which is a time for magnetic heads to reach desired location.

% Above is a great example of how file system has to be flexible to solve such problems as well as of problems that emerge after running file system for a longer period of time.

% The ideal approach for research pointed on effect on age of file system would be to put file system under defined conditions for a few years or months, gathering information in the process. 

% However, this is apparently impractical or even impossible, because the demand has to be satisfied in matter of weeks to respond to new versions of file systems, which are developed and released quite often.

One of the earliest ideas about how to deal with this problem would be to capture a snapshot of a file system that had already been used for a prolonged time. 

This approach of testing on such a snapshot, however, would probably lead to optimising for a very specific instance of old version of given file system. 

If researchers want to predict behavior of a new version of file system after prolonged time of usage, they should be able to create a simulation, which would mimic the aging process, but in short period of time.

This can be technically implemented, because the idea is to run the simulation continuously under very heavy workload, which would not have any delays. We believe, that file systems used in real life are not really used continuosly, therefore researchers should be able to compress the time of creating such an aged file system in matter of days.

Unfortunately the demand from users and developers has yet not been met by standard studies. There has been few studies executed in last millenium[3], that shed some light on this topic, but overall, modern research is in hands of regular users, which remain in amateur sphere, lacking professional equipment to execute such research properly.

My general idea is to design a set of heavy workloads, simulating aging process, to run on different types of devices and at least two types of file systems and then to execute a series of performance tests. Results of these tests could be then compared to fresh file systems, showing performance differences between them. Comparing different aged file systems between each other would clearly be an interesting topic of research as well.

\chapter{File systems and used tools}
Possible other sections to explain terms: journaling filesystem, allocation groups, B+ trees
\section{File systems}
File system is a set of tools, methods, logic and structure to control how to store and retreive data on and from a storage, e.g. device. It is sometimes called a 'bookkeeper' of operational system. As an  analogy to paper-based systems. Basic user-accesed units are called files, which could be clustered into directories.

The system stores files either continuously or scattered across device. The basic accessed data unit is called a block, which capacity can be set to various sizes. Blocks are labeled either as free or used.

Files which are non-continous are stored in form of extents, which is one or more blocks associated with the file, but stored elsewhere.  

Information about how many blocks does a file occupy, as well as other information like date of creation, date of last access or access permissions is known as metadata, e.g. data about stored data. This information is stored separately from the content of files. On modern file systems, metadata are stored in objects called inodes (index nodes). Each file a file system manages is associated with an inode and every inode has its number in an inode table. On top of that the file system stores metadata unrelated to any specific file, such as information about bad sectors, free space or block availability.

% Depending on file system, these metadata are stored in various ways (different tables or arrays), but on modern file systems, there are objects called inodes (i stands for index) which content is separate from the contents of files. Each file a file system manages is associated with an inode, every inode has its number in an inode table.

*este nieco o bitovych mapach volneho miesta.

In this thesis, targeted file systems will be UNIX XFS and EXT4, which are main Red Hat supported file systems. These file systems belong to the group of journaling file systems.

Journaling file system keeps a structure called journal, which is a buffer of changes not yet commited to the file system. After system failure, these planned changes can be easily read from the journal, thus making the file system easily fully operational, and in correct and consistent state again.

*IO scheduler
\section{XFS}
XFS is a 64-bit journaling file system created by Silicon Graphics, Inc(SGI) in 1993. It is known for great performance in execution of paralel I/O operations, because of its architecture based on allocation groups.

Allocation groups are euqally sized linear regions within file system. Each allocation group manages its own inodes and free space, therefore increasing parallelism.
Architecture of this design enables for significant scalability of bandwidth, threads, and size of file system, as well as files, simply because multiple processes and threads can access the file system simultaneously.

XFS allocates space as extents stored in pairs of B+ trees, each pair for each allocation group (improving performance especially when handling large files). One of the B+ trees is indexed by the length of the free extents, while the other is indexed by the starting block of the free extents. This dual indexing scheme allows for the highly efficient location of free extents for file system operations.

Prevention of file system fragmentation consist mainly of a feature called \textit{delayed allocation} as well as online defragmentation(\textit{xfs\_fsr}), that can turururu

Delayed allocation, also called \textit{allocate-on-flush} is a feature that, when a file is written to the buffer cache, substracts space from the free-space counter, but won't allocate the free-space bitmap. The data is held in memory until it have to be stored because of system call (such as \textit{sync}). This approach improves the chance, that the file will be written in a contiguous group of blocks, avoiding fragmentation and reducing CPU usage as well.

\section{EXT4}
Ext4, also called fourth extended filesystem is a 48-bit journaling file system developed as successor of ext3 for Linux kernel, improving reliability and performance features.

Traditionally, ext* systems use an indirect block mapping sheme. Such approach is generaly inefficient for large files when using operations like deleting or truncating. Ext4, as well as XFS use approach of \textit{extents}, which positively affect performance and encourage ?continuous layouts?.

When allocating, ext4 use multiblock allocation. 
\todo{?what is multiblock allocation?}

Similary as xfs, ext4 use delayed allocation to increase performance, especially when in use with multiblock allocation and extent-based approach, also reducing fragmentation on the device. For cases of fragmentation that still occur, ext4 provide support for online defragmentation and \textit{e4defrag} tool to defragment either single file, or whole file system.

\section{FIO}
Flexible Input/Output tool is a IO workload generator written by Jens Axboe. It is a tool well known for it's flexibility as well as large group of contributors and users.


\section{Fs-drift}
fs-drift is a workload aging test written by Ben England. It relies on randomly mixed requests generated by inner heuristic (according to parameters). These requests can be writes, reads, creates, appends or deletes.

At the beginning of run time, the top directory is empty, and therefore \textit{create} requests success the most, other requests, such as \textit{read} or \textit{delete}, will fail because of lack of files and small probability of randomly choosing existing one. 

Over time, as the file system grows, \textit{create} requests began to fail and other requests succede more. Finally, file system will reach a state of equilibrium, when requests are equaly likely to execute. From this point, the file system would not grow anymore, and the test runs until one of the \textit{STOP} conditions are met (specified with parameters).

Fs-drift is very flexible and can be used to simulate lots of different workloads by operating with various file sizes, request types and different kinds of random distribution.

\section{File system images}
To achieve consistency of results and to shorten testing time, file system images are used. Once the image is created, it can be stored for later use and replayed back on device. To save space, only metadata of created file system are used, since content of created files is random and therefore irrelevant. Replayed metadata point at various blocks on device, recreating fragmentation while seldom taking significantly less space. These images can be created by using tools developed to inspect file systems in case of emergency. For ext based file systems, there is e2image tool and for xfs, there is xfs\_metadump. Both tools create images as sparse files, so compression is needed.

E2image tool can save whole ext based file system or just its metadata and offers compresion of image as well. Created images can be further compressed by tools such bzip2 or tar.

\noindent Creating compressed image using e2image:
\begin{lstlisting}[language=bash]
  e2image -Q $DEVICE $NAME.qcow2
\end{lstlisting}

\noindent Such images can be later replayed back on a device. From that point, file system can be mounted and revised.

\noindent Replaying compressed image:
\begin{lstlisting}[language=bash]
  e2image -r $NAME.qcow2 $DEVICE
\end{lstlisting}

Xfs\_metadump saves XFS file system metadata to a file. Due to privacy reasons file names are obsfucated (can be disabled by -o parameter). As well as e2image tool, the image file is sparse, but xfs\_metadump doesn't offer a way to compress the output. However, output can be redirected to stdout from where it can be passed to a compression tool. Creating compressed image using xfs\_metadump:

\begin{lstlisting}[language=bash]
  xfs_metadump -o $DEVICE -|bzip2 > $NAME
\end{lstlisting}

Such images, when uncompressed can be replayed back on device by tool xfs\_mdrestore. File system can be then mouned and inspected as needed:

\begin{lstlisting}[language=bash]
  xfs_mdrestore $NAME $DEVICE
\end{lstlisting}

\chapter{Storage media}
\section{SATA}
\section{SAS}
\section{HDD}
\section{SDD}
\chapter{Implementation}
\section{filesystem\_ager}
%This aging tool is a simple approach to write and remove many files of random size.

%The tool consist of three scripts and one common library called \textit{functions}. The scripts are named \textit{filesystem\_ager.py}, \textit{fio\_config\_generator.py} and \textit{random\_deletor.py}.

%The workflow consist of calling filesystem\_ager, with desired parameters. Script manages triggering fio\_config\_generator, calling fio tool on generated config and triggering random deletor. These three actions are repeated given number of times.
%Parameters of filesystem\_ager are: 
%\begin{compactenum}
%  \item Total desired size do be written in one cycle
%  \item Denominator of total desired size (Total desired size will be divided by this number)
%  \item Range of size of written files
%  \item Number of cycles
%\end{compactenum}

%Although FIO tool has some parameters to randomize the size of files which are written, the management of file sizes and randomisation, as well as naming of files is handled by fio\_config\_generator instead, to provide more control over those qualities.
%Parameters of this script are:
%\begin{compactenum}
%  \item Total deisred size to be written
%  \item Range of size of written files
%\end{compactenum}

%The script will generate global settings of a workload, then proceeds to generate jobs for every file that will be written. File size is always the name of that file, and these are gathered to a list, then list of generated files is returned and script ends. Including file size in its name, as well as indexation of files will help effectively search and delete files in the random deletion process, without need to search for files on the disk and examine them for size. Simplistic approach in fio config will hopefully result in compatibility and reliability in use with any fio version.

%After config file is generated, filesystem\_ager will run fio tool on generated config and therefore, files are written on the device.

%The removing of files is handled by random\_deletor script. Its parameters are:
%\begin{compactenum}
%  \item Total written size
%  \item Denominator of total size
%  \item Range of size of written files
%  \item Number of existent files
%\end{compactenum}

%If denominator equals zero, random\_deletor wont remove any files and will return empty list. Otherwise, desired range of deletion is estimated. Random\_deletor then proceeds to remove files while desired volume is not deleted. Files are randomly selected through choosing random integer from zero to number of existent files. This step may seem inefficient, but with large amounts of generated files, the time to perform succesfull selection will not change dramatically. Selected file name is then parsed for size information, and if it fits into desired volume to remove, it is deleted, through subprocess command. Names of removed files are gathered in a list and returned.

%Number of deleted files is substracted from number of existent files.
%filesystem\_ager then sums up deleted volume, log it as well as other information and triggers the cycle again.

%However, after few runs, I decided not to use this approach for actual aging, because the time needed to fill and appropriately age the filesystem simply took very long. Instead, I was looking for other, already created tools I could use.


\section{Performance testing of images}
Performance testing of created images is done by a package recipe\_fio\_aging. Upon instalation, the package finds and downloads coresponding file system image according to obtained parameters. As shown, images are stored compressed, therefore decompression is needed after download. Once these steps are succesfully completed, the image can be replayed on the device by using presented tools (e2image, xfs\_mdrestore).


Measuring a performance is done by a tool I developed, recipe\_fio. Similar to filesytem\_ager, recipe\_fio use fio tool to handle desired IO operations, but instead of focusing on filling the filesystem, the script use measurement features of fio, which consist of performing IO operations and reporting results.

*talk about used recipe

For purposes of this thesis, I let recipe\_fio to report bandwidth and operations per second(IOPS).

The main script receives slightly enhanced fio configuration file, enriched of some non-fio parameters, which are used by test only. These parameters are:
\begin{compactenum}
  \item used filesystem
  \item number of test repetitions. For statistical stability, I decided to run the test several times under same conditions.
  \item specifying a snapshot to be tested
  \item flag which represents whether or not to rsync data on a result-storing server
  \end{compactenum}

After compiling, tool parses the parameters and gathers information about system, which consist of: version of kernel, time and date, hostname, RHEL compose, memory, kernel, mount, system info, system variables and fio version.

Then it proceeds to set environment for testing by:

\begin{compactenum}
  \item Installing fio tool
  \item Creating directory for results
  \item Loading given snapshot on a device
  \item Randomly removing volume from the device to make room for test
\end{compactenum}

Volume is randomly removed using random\_delete\_volume.py script. This scriptt globs all files in the filesystem, retrieves information about used volume as well as it's overall size. Then proceeds to randomly choosing files to delete and stops when desired volume is freed. The approach of recursively globbing all files may be inefficient, but this way, we can be sure, that volume is deleted from whole device evenly.

Python script run\_tests.py, manages to parse recipe parameter, resulting in creating one or several fio configuration files in the directory, further adding logging parameters to them. Then for every created file, directory on the desired medium is created and fio tool is triggered. If the script succesfully ends, file OK is created in the results directory.

When the measurement is over, bash script generate the name of results, which consist of:
\begin{compactenum}
  \item time
  \item date
  \item used snapshot
  \item version of kernel
  \item version of RHEL compose
\end{compactenum}

Then proceeds to compress results into tar archive with generated name, and according to -g flag will, or will not, rsync the result onto the data server.
\section{Inspecting filesystem}
For determining some overall idea about an extent to which is the filesystem aged or dirty, I wrote scripts that generate histograms representing fragmentation of used space as well as fragmentation of free space. Both scripts use common linux tools and pyplot to generate the graphics. Both scripts can display linear or logarhytmic Y scale.


Script extent\_distribution.py makes use of xfs\_io fiemap tool, which is a tool to display extent distribution of a given file and works correctly even for ext* filesystems.

The script will first recursively crawls the whole filesystem from given top folder and makes a list of all files. Fiemap is then run over every file separately. 

The only data, that are then parsed from the output, is how many non-contigous extents does the file have. These integers are aggregated to a single list, from which are then counted, and final histogram is made.

Script free\_space\_fragmentation.py use the tool e2freefrag, which runs over a device, and outputs the histogram of free space fragmentation in texutal form. Script will store this output and then easily parse the histogram and aggregate the data into a graphic form.

\section{Aging recipes}
To determine which fs-drift settings will be the most fitting for purposes of this thesis, I wrote a small python script fs-drift\_matrix.

It is capable of taking matrix of possible fs-drift paramteres from \textit{json} file and then run it on a device.

After every run, histograms-generating scripts are triggered to store histograms of \textit{free space} and \textit{used space} fragmentation. Also outputs of the \textit{fs-drift} script and \textit{df} command are logged.

As the creator states in README, to fill up a filesystem, maximum number of files and mean size of file should be defined such that the product is greater than the available space.

For the purpose of this thesis, desired usage is 60\%-100\% with enough fragmentation to consider the device aged.

\section{Data processing}
\chapter{Testing environment}
The aging process took place on an ibm3250 machine with following parameters:
\begin{compactenum}
  \item Intel(R) Xeon(R) CPU, X2460 (2.80Ghz, ??? Cache,8 cores)
  \item RAM 10GB ???Mhz DDR? ???
  \item 4x300GB SAS disks + 1x50GB Sas disk ???
\end{compactenum}

THIS IS AN EXAMPLE, HOW SHOULD SCECIFICS OF A MACHINE LOOK

2 x Intel Xeon E5-2620 (2.0GHz, 15MB Cache, 6-cores)
Intel C602 Chipset
Memory - 16GB (2 x 8GB) 1333Mhz DDR3 Registered RDIMMs
 CentOS 6.3 64-Bit
100GB Micron RealSSD P400e Boot SSD
LSI 9207-8i SAS/SATA 6.0Gb/s HBA (For benchmarking SSDs or HDDs)

The system installed on machine was ----- with kernel 3.10.0-229.el7.x86\_64 

I created a two volume groups, G1 and G2. Each group have a pair of 300GB disks with striping of 2. This setting allows to double the speed of IO operations.

On each volume group i created logical volume of 100GB.
\section{HDD and SSD}
HDD is a rotational disk, which requires specific approach from kernel, to ensure the lowest possible seek time. Seek time is a time for moving parts of the device to find next relevant block of data. This affect overall performance greatly, because with large fragmentation, seek time becomes quite high.

As for SSD, this type of device does not have any moving parts, which make perform really well. One of the problems, however, is limited lifecycle of memory cells. SSD manufacturers deal with this problem by adding controler with its own scheduler, which make sure, no parts of the device are used significantly more than other parts.

When aging the filesystems, I expect for those grown on HDD to perform significantly slower after aging process, and I expect SSD filesystems not to be affected at all, or maybe significantly less.

\chapter{Results}
The output of result generator is a htlm report summarising all information about system, links to raw data and charts of measured values.
*talk about highcharts and how do you represent data

\section{Comparing against fresh filesystems}
\section{Comparing performance of Ext4 and XFS}
\section{Comparing overall state of aged EXT4 and XFS}
\section{Comparing rotational and solid state disks}
\chapter{Conclusion}
Here I will admit, that these results were not really surprising and ABSOLUTELY no breakthrough, however, as noone really research this branch of QE, the results are definitely a step further in this field.
\end{document}
