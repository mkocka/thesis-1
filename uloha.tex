\documentclass[color]{fithesis3}
\usepackage{natbib}
\usepackage{beamerarticle}
\AtBeginDocument{
  \let\subsection\section
  \let\section\chapter
  \let\tableofcontents\relax}
\usepackage[
  main=english, %% By using `czech` or `slovak` as the main locale
                %% instead of `english`, you can typeset the thesis
                %% in either Czech or Slovak, respectively.
  german, russian, czech, slovak %% The additional keys allow
]{babel}  



\title{The effect of age on file systems}
\subtitle{State of art}
\author{Samuel Petrovic}

\begin{document}
\chapter{State of art}
We live in the age of information. Hardware and software technology improve every day. There is a need to work with large databases anda multi-media applications as well as to store great amount of data on a memory device. This cause great pressure on performance of storing and retrieving information, viz. input and output(I/O) operations.

The part of an operation system that handles communication with a physical device is called a file system. Because of such increasing requirement on performance of I/O operations, there has been quite wide technological progress with respect to performance issues. 

Over the past few years, great many tools and tests (e.g. benchmarks) were developed as means for users and researchers to explore behavior and measure performance of file systems. These usually consist of putting file system under some kind of stress, gathering different measurements in the process and computing overall summary after the test is done. Measured parameters are usually bandwidth, latency and IO operations per second.

The benchmark should be highly configurable, so the researchers could simulate various workloads that would mimic real life scenarios. Stability and reproducibility of tests are also required, if the research is meant to be objective and to have some academic value about file system itself. ~\cite{rocketscience:qhe}

Configuration of used benchmark is highly crucial for research to have realistic result. However, large amount of benchmarks are flawed or arbitrary in nature, thus are impossible to configure to provide realistic measurements~\cite{fsbench-article:qhe}.

The standard workflow is to run benchmark on a clean instance of operation system without other applications running in order to avoid noise, and on clean instance of researched file system. 

While this approach brings great results, with correct configuration of the benchmark, it only gives us a general idea about how the file system performs in early stage of its usage. 

However, there is a growing demand from users as well as from developers to explore behavior and performance of a file system, that has been running under some defined conditions for longer period of time, months or even years ~\cite{harvardaging:qhe}.

Prolonged usage of file system impels it to do more and more optimisations. When there is a lot of free space, there is generally no need for complicated approach. Nevertheless, while more files are deleted, created and expanded, the free space becomes increasingly fragmented. This then leads for files to be spread non contiguously across the physical device.

This problem concerns particulary devices with moving parts, such as the widely used hard disk drive. When working with IO operations on this type of device, the fragmentation causes longer seek times, which is the time needed for magnetic heads to reach the desired location, therefore slowing down the file system ~\cite{windows2006:qhe}.

Above is a great example of how file system has to be flexible to solve such problems as well as of problems that emerge after running a file system for a longer period of time.

The ideal approach for research focused on monitoring the effects of file system aging would be to put the file system under defined conditions for a few years or months, gathering information in the process. 

However, this is apparently impractical or even impossible, because the demand has to be satisfied in the course of few weels to respond to the new versions of file systems, which are developed and released quite often.

One of the earliest ideas about how to deal with this problem would be to capture a snapshot of a file system that had already been used for a prolonged time. 

However, this method would probably lead to optimising for a very specific instance of old version of the given file system. 

If researchers want to predict behavior of a new version of file system after prolonged time of usage, they should be able to create a simulation, which would mimic the aging process, but in short period of time.

This can be technically implemented, because the idea is to run the simulation continuously under very heavy workload, without any delays. We believe that file systems used in real life are not really used continuosly, therefore researchers should be able to compress the time of creating such an aged file system in a few days.

Unfortunately users and developers demand has yet not been met by standard studies. Few studies have been executed in last millenium ~\cite{harvardaging:qhe} that would shed some light on this topic but in general, modern research is in hands of regular users, which remain in amateur sphere, lacking professional equipment to execute such research properly.

My general idea is to design a set of heavy workloads, simulating the aging process, to run on different types of devices and at least on two types of file systems and then to execute a series of performance tests. The results of these tests could be compared then to fresh file systems, showing performance differences between them. Comparing different aged file systems clearly be an interesting topic of research as well.


\bibliography{qhe}
\bibliographystyle{plain}
\end{document} 


